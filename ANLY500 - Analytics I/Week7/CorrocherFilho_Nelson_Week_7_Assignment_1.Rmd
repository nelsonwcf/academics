---
title: "San Francisco Bay Guardian Paper Text Mining"
author: "Nelson Corrocher"
date: "August 21, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- This section install the required packages -->
```{r needed_packages, echo = FALSE, include = FALSE}
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc") 
install.packages(Needed, dependencies=TRUE, repos = "http://datacube.wu.ac.at/")   
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source") 
```

#### The following section prepare the corpus of the text mining. In this exercise, the folder texts/ is being used so the text object must be put in that folder.
```{r prep_env, echo = FALSE}
setwd("C:/Users/Nelson Corrocher/WorkSpace/R/Week7") # <<<< Set your working environment here
cname <- file.path(getwd(), "texts")
cname
dir(cname)
library(tm) # tm is the text mining package
docs <- Corpus(DirSource(cname)) # Set up the Corpus on docs
```

#### The following section pre-process the words (removes punctuation and number, convert to lwoercase, remove stopwords, extract the word stem and remove whitepsaces).
```{r text_pproc}
docs <- tm_map(docs, removePunctuation)   # *Removing punctuation:*    
docs <- tm_map(docs, removeNumbers)      # *Removing numbers:*    
docs <- tm_map(docs, tolower)   # *Converting to lowercase:*    
docs <- tm_map(docs, removeWords, stopwords("english"))   # *Removing "stopwords" 
library(SnowballC)   
docs <- tm_map(docs, stemDocument)   # *Removing common word endings* (e.g., "ing", "es")   
docs <- tm_map(docs, stripWhitespace)   # *Stripping whitespace   
docs <- tm_map(docs, PlainTextDocument)
```

#### Staging the Data (as Document-Term Matrix)
```{r staging}
dtm <- DocumentTermMatrix(docs) 
# tdm <- TermDocumentMatrix(docs) This line is commented because it is not used
```

#### The following section is used for some data familiarization and to catch evident outliers:
```{r explor}
freq <- colSums(as.matrix(dtm))   
length(freq)   
ord <- order(freq)   
m <- as.matrix(dtm)   
dim(m)   
write.csv(m, file="DocumentTermMatrix.csv") # The excel file in the working directory now holds word counting
```

#### Some cleaning up of the data below. The output shows two rows of numbers. The top number is the frequency with which words appear and the bottom number reflects how many words appear that frequently:
```{r clean}
#  Start by removing sparse terms:   
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.   
head(table(freq), 20) ### Word Frequency 
tail(table(freq), 20)
```

#### Next, let's consider only biggest frequencies (in this example, 42):
```{r top_freq}
freq <- colSums(as.matrix(dtms))  # Matrix is too big to be shown on this document
f_list <- findFreqTerms(dtm, lowfreq=30)   # <<<< Change it to the most appropriate for the data
f_list
```

#### Ploting Word Frequencies (Only words that appear at least 50 times):
```{r plot, echo = FALSE}
library(ggplot2)   
wf <- data.frame(word=names(freq), freq=freq)   
p <- ggplot(subset(wf, freq>50), aes(word, freq))    
p <- p + geom_bar(stat="identity", colour = "white", fill = "dark red")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p
```

#### Next, we can try to find the correlation between terms that occurs in the corpus. In this context, correlation is a quantitative measure of the co-occurrence of words in multiple documents. Since we have only one document in this corpus, it will always return 0. This, it was set not to show anything. 
```{r correl_test, results='hide'}
findAssocs(dtm,f_list ,corlimit = 0.0) # <<<< Adjust the corlimit to the desired correlation level. Obs: Since there is only one document, nothing will be shown.
```

#### Here we create a word cloud based on the frequency of the words. 
```{r clouds, echo = FALSE}
library(wordcloud)   
dtms <- removeSparseTerms(dtm, 0.15) # Prepare the data (max 15% empty space)   
freq <- colSums(as.matrix(dtm)) # Find word frequencies   
dark2 <- brewer.pal(6, "Dark2")   
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)    
```

#### The code below shows a way to measure similarity between documents in the corpus. In this activity, however, we have only one document and we can really compute the d (similarity distance between documents) so this portion of the code will be commented.
```{r clust_eucl}
#dtms <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space.
#library(cluster)   
#d <- dist(t(dtms), method="euclidian")   # First calculate distance between words
#fit <- hclust(d=d, method="ward.D")   
#plot.new()
#plot(fit, hang=-1)
#groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using   
#rect.hclust(fit, k=5, border="red") # draw dendogram with red borders around the 5 clusters   
```

#### Same case as before, but using k-means algorithms. It also has to be commented out.
```{r k-means}
#library(fpc)   
#library(cluster)  
#dtms <- removeSparseTerms(dtm, 0.15) # Prepare the data (max 15% empty space)   
#d <- dist(t(dtms), method="euclidian")   
#kfit <- kmeans(d, 2)   
#clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```